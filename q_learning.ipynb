{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "q_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN2hsIuFm5jH6BeIh+xwo1g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pereirarodrigo/q_learning/blob/main/q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-8I1C2LivDv"
      },
      "source": [
        "# Reinforcement learning: Q-learning applied to a self-driving taxi cab\n",
        "-------------\n",
        "> This is a notebook dedicated to Satwik and Brendan's article on Reinforcement Learning, named \"Reinforcement Q Learning from scratch in Python with OpenAI Gym\" (https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/).\n",
        "\n",
        "> The idea of this tutorial is to apply Q learning to a self-driving taxi cab, whose objective is to pick up 4 passengers in a 5x5 grid."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qizgzoCfG_x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "369fe93a-8795-47cf-b434-a716586fe600"
      },
      "source": [
        "# Installing OpenAI Gym, a library of game environments\n",
        "\n",
        "!pip install cmake 'gym[atari]' scipy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cmake in /usr/local/lib/python3.6/dist-packages (3.12.0)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.19.4)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (7.0.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari]) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kI09k-O2fzoG",
        "outputId": "a80bb6f9-0759-4f75-d0d8-3b717c5de042"
      },
      "source": [
        "# Loading the game environment and rendering it\n",
        "# Note: Taxi-v2 has been superseded by Taxi-v3\n",
        "\n",
        "import gym\n",
        "\n",
        "env = gym.make(\"Taxi-v3\").env\n",
        "\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vU8aqCRgkSXo"
      },
      "source": [
        "## The problem\n",
        "--------\n",
        "The taxi problem is written as follows (from the official Gym docs):\n",
        "\n",
        "<br>\n",
        "\n",
        "> \"There are 4 locations (labeled by different letters), and our job is to pick up the passenger at one location and drop him off at another. We receive +20 points for a successful drop-off and lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\"\n",
        "\n",
        "<br>\n",
        "\n",
        "Additionally, we have the following information:\n",
        "\n",
        "<br>\n",
        "\n",
        "*   The filled square represents the taxi. If it has no passenger, it is yellow; otherwise, it's green;\n",
        "*   The pipe (|) represents a wall, which the taxi cannot pass;\n",
        "*   R, G, Y, B are the possible pickup and destination locations. The blue letter represents the current passenger pick-up location, and the purple letter is the current destination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBgqTqLRilrH",
        "outputId": "481fa71b-5c72-44d6-d17e-34a14930b197"
      },
      "source": [
        "env.reset() # resetting the environment to a new, random state\n",
        "env.render()\n",
        "\n",
        "print(f\"Action state: {env.action_space}\")   # 6 action spaces (0 = south, 1 = north, 2 = east, 3 = west, 4 = pickup, 5 = dropoff)\n",
        "\n",
        "print(f\"State space: {env.observation_space}\") # 500 state spaces (5 passengers * 5x5 taxi locations * 4 destinations)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : :\u001b[43m \u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[34;1mB\u001b[0m: |\n",
            "+---------+\n",
            "\n",
            "Action state: Discrete(6)\n",
            "State space: Discrete(500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pt39x47ApKUi",
        "outputId": "a37db94c-b625-4f10-b232-a6a6b4379aa6"
      },
      "source": [
        "# Using the illustration's coordinates to set the environment's state, which is between 0 and 499\n",
        "# In this case, the state is 209\n",
        "# It is possible to set the environment's state manually with env.env.s using that number  \n",
        "\n",
        "state = env.encode(2, 0, 2, 1)  # Taxi is at row 2 and column 0, passenger is at 2 and the destination is location 1 (R = 0, G = 1, Y = 2, B = 3)\n",
        "\n",
        "print(\"State: \", state)\n",
        "\n",
        "env.s = state  # setting the state \n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "State:  209\n",
            "+---------+\n",
            "|R: | : :\u001b[35mG\u001b[0m|\n",
            "| : | : : |\n",
            "|\u001b[43m \u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0TOcYKhrWvF",
        "outputId": "9f274587-bdd2-441a-b25c-558d6e81abcd"
      },
      "source": [
        "# Showing the reward table for state 209\r\n",
        "# This table is created when the taxi environment is created\r\n",
        "# It can be thought of as a states x actions matrix\r\n",
        "# Its structure is: {action: [(probability, nextstate, reward, done)]}\r\n",
        "# In our environment, the probability will always be 1 (100%)\r\n",
        "# The nextstate is the state we would be in if we take the action at this index of the dictionary\r\n",
        "# All movements have a reward of -1, while pickup/dropoff have a reward of -10\r\n",
        "# If the taxi has a passenger and is located in a state with the right dropoff location, the total reward is 20\r\n",
        "# Done tells us when the taxi has successfully dropped off a passenger in the right location. Each successful dropoff is the end of the episode\r\n",
        "\r\n",
        "env.P[209]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 309, -1, False)],\n",
              " 1: [(1.0, 109, -1, False)],\n",
              " 2: [(1.0, 229, -1, False)],\n",
              " 3: [(1.0, 209, -1, False)],\n",
              " 4: [(1.0, 209, -10, False)],\n",
              " 5: [(1.0, 209, -10, False)]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_S50ZYPTs9xP"
      },
      "source": [
        "## Solving the problem without Reinforcement Learning\r\n",
        "----------\r\n",
        "It's possible to brute-force our way through this problem and solve it without using Reinforcement Learning. Given we have our $P$ table of default rewards in each state, we can make our taxi navigate by using only that. However, we will soon see that this approach is abysmally inefficient - the agent will make a lot of wrong dropoffs and take too long to do its task.\r\n",
        "\r\n",
        "The way we do this is through an infinite loop which runs until one passenger reaches their destination, thus completing one episode, which is when the received reward is 20. The ```env.action_space.sample()``` method automatically selects one random action from the set of all possible actions. With this in mind, let's see what happens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aASAWJk_s8ka",
        "outputId": "8eb29183-4963-4cf8-ad32-78db5d37aaaf"
      },
      "source": [
        "env.s = 209  # manually setting the environment\r\n",
        "epochs = 0\r\n",
        "penalties, reward = 0, 0\r\n",
        "\r\n",
        "frames = [] # for the animation\r\n",
        "done = False\r\n",
        "\r\n",
        "while not done:\r\n",
        "  action = env.action_space.sample()\r\n",
        "  state, reward, done, info = env.step(action)\r\n",
        "\r\n",
        "  if reward == -10:\r\n",
        "    penalties += 1\r\n",
        "\r\n",
        "  # Putting each rendered frame into a dict for the animation\r\n",
        "\r\n",
        "  frames.append({\r\n",
        "      'frame' : env.render(mode = 'ansi'),\r\n",
        "      'state' : state,\r\n",
        "      'action' : action,\r\n",
        "      'reward' : reward\r\n",
        "      }\r\n",
        "    )\r\n",
        "\r\n",
        "  epochs += 1\r\n",
        "\r\n",
        "print(f\"Timesteps taken: {epochs}\") \r\n",
        "print(f\"Penalties incurred: {penalties}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Timesteps taken: 315\n",
            "Penalties incurred: 92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w6QRFCO0xSR",
        "outputId": "a64d5bbe-cda5-4897-a06b-bf34b44be34f"
      },
      "source": [
        "from IPython.display import clear_output\r\n",
        "from time import sleep\r\n",
        "\r\n",
        "def print_frames(frames, speed = .1):\r\n",
        "  for i, frame in enumerate(frames):\r\n",
        "    clear_output(wait = True)\r\n",
        "    \r\n",
        "    print(frame.get(\"frame\"))\r\n",
        "    print(f\"Timestep: {i + 1}\")\r\n",
        "    print(f\"State: {frame['state']}\")\r\n",
        "    print(f\"Action: {frame['action']}\")\r\n",
        "    print(f\"Reward: {frame['reward']}\")\r\n",
        "\r\n",
        "    sleep(speed)\r\n",
        "\r\n",
        "print_frames(frames)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[35m\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Timestep: 315\n",
            "State: 85\n",
            "Action: 5\n",
            "Reward: 20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXcz1Ne93gZz"
      },
      "source": [
        "## Using Q-learning for a better solution\r\n",
        "--------\r\n",
        "Q-learning is a simple Reinforcement Learning algorithm that allows an agent to use its environment's rewards in order to learn, over time, the best action to take in a given state.\r\n",
        "\r\n",
        "In our taxi environment, $P$, the reward table, is the means by which the agent will learn about solving its task. It does this by receiving a reward from for taking an action in the current state, then updating a Q-value to remember if that action was beneficial. Q-values are valures store in a Q-table, and they map to a ```(state, action)``` combination.\r\n",
        "\r\n",
        "A Q-value for a particular state-action combination represents the \"quality\" of an action taken in that state. Thus, better Q-values imply better chances of getting better rewards. For example, if the taxi is faced with a state that includes a passenger at its current location, the Q-value for ```pickup``` will be significantly higher when compared to other actions, such as ```dropoff``` or moving to another direction.\r\n",
        "\r\n",
        "Q-values are initialized to an arbitrary value (such as 0) and, as the agent exposes itself to the environment and receives different rewards by executing different actions, the Q-values are updated using the equation:\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "$$Q(state, action) \\leftarrow (1 - \\alpha)Q(state, action) + \\alpha(reward + \\gamma \\: \\underset{\\alpha}{\\mathrm{max}}\\:Q(\\text{next state, all actions}))$$\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "Where:\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "*    $\\alpha$ = learning rate ($0 < \\alpha \\leq 1$), similar to supervised learning. It's the extent to which the Q-values are being updated;\r\n",
        "\r\n",
        "*   $\\gamma$ = discount factor ($0 \\leq \\gamma \\leq 1$), which determines how much importance we want to give future rewards. A high value for the discount factor (close to 1) captures the long-term effective award, whereas a smaller factor value (close to 0) makes our agent consider only the immediate reward, thus making it greedy.\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "We are assigning ($\\leftarrow$), or updating, the Q-value of the agent's current state and action by first taking a weight ($1 - \\alpha$) of the old Q-value, then adding the learned value, which is a combination of the reward for taking the current action in the current state. The discounted maximum reward from the next state will be in once we take the current action.\r\n",
        "\r\n",
        "After obtaining our Q-values, we need to store them in a Q-table, which has a row for every state (in our case, 500) and a column for every action (6). Its values are initially 0 and get updated after training. Our Q-table looks like this:\r\n",
        "\r\n",
        "[Inserting an image]: ![](https://storage.googleapis.com/lds-media/images/q-matrix-initialized-to-learned_gQq0BFs.width-1200.png)\r\n",
        "\r\n",
        "<center><img src = \"https://storage.googleapis.com/lds-media/images/q-matrix-initialized-to-learned_gQq0BFs.width-1200.png\" width = \"600\" height = \"600\">\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "<font size=\"3\"> Source: https://storage.googleapis.com/lds-media/images/q-matrix-initialized-to-learned_gQq0BFs.width-1200.png</font></center>\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "Overall, the steps required for Q-learning are:\r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "1.   Initialize the Q-table with 0;\r\n",
        "2.   Start exploring actions: for each state, select any one among all possible actions for the current state $S$;\r\n",
        "3.   Travel to the next state $S'$ as a result of action $a$;\r\n",
        "4.   For all possible actions from the state $S'$, select the one with the highest Q-value;\r\n",
        "5.   Update Q-table values using the equation;\r\n",
        "6.   Set the next state as the current state;\r\n",
        "7.   If the goal state is reached, then end and repeat the process. \r\n",
        "\r\n",
        "<br>\r\n",
        "\r\n",
        "With this information at hand, it's time to implement Q-learning in Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq1WNvgvV-b_"
      },
      "source": [
        "# Creating a 500x6 matrix of zeroes\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWMuNL6CWadu",
        "outputId": "280da937-22bd-4e6b-eed9-2c4f0c44d6f2"
      },
      "source": [
        "%%time\r\n",
        "\r\n",
        "# Training the algorithm\r\n",
        "\r\n",
        "import random\r\n",
        "from IPython.display import clear_output\r\n",
        "\r\n",
        "# Hyperparameters\r\n",
        "\r\n",
        "alpha = 0.1\r\n",
        "gamma = 0.6\r\n",
        "epsilon = 0.1  # balancing exploration vs. exploitation, or choosing randomly vs. following known information\r\n",
        "               # the smaller epsilon is, the more the agent will explore (thus resulting in more penalties)\r\n",
        "\r\n",
        "# For plotting metrics\r\n",
        "\r\n",
        "all_epochs = []\r\n",
        "all_penalties = []\r\n",
        "\r\n",
        "for i in range(1, 100001):\r\n",
        "  state = env.reset()\r\n",
        "\r\n",
        "  epochs, penalties, reward, = 0, 0, 0\r\n",
        "  done = False\r\n",
        "\r\n",
        "  while not done:\r\n",
        "    if random.uniform(0, 1) < epsilon:\r\n",
        "      action = env.action_space.sample()  # explore the action space\r\n",
        "\r\n",
        "    else:\r\n",
        "      action = np.argmax(q_table[state])  # exploit the learned value\r\n",
        "\r\n",
        "    next_state, reward, done, info = env.step(action)\r\n",
        "\r\n",
        "    old_value = q_table[state, action]\r\n",
        "    next_max = np.max(q_table[next_state])\r\n",
        "        \r\n",
        "    new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\r\n",
        "    q_table[state, action] = new_value\r\n",
        "\r\n",
        "    if reward == -10:\r\n",
        "      penalties += 1\r\n",
        "\r\n",
        "    state = next_state\r\n",
        "    epochs += 1\r\n",
        "        \r\n",
        "    if i % 100 == 0:\r\n",
        "      clear_output(wait = True)\r\n",
        "      print(f\"Episode: {i}\")\r\n",
        "\r\n",
        "print(\"Training finished.\\n\")\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n",
            "CPU times: user 1min 38s, sys: 7.16 s, total: 1min 45s\n",
            "Wall time: 2min 20s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWrb_0WMZS0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d4154d6-52c0-47b2-d9f2-9c172b4d88ec"
      },
      "source": [
        "# Checking the Q-values at our illustration's state\r\n",
        "# The max Q-value is \"south\" (-2.418), which means that the agent has succeeded in learning\r\n",
        "\r\n",
        "q_table[209]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -2.41837066,  -2.47061343,  -2.47061344,  -2.45102239,\n",
              "       -11.45101854, -11.45102188])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHtyGvxPaXh6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a3802b3-6565-40e7-f0cb-c1a45018f477"
      },
      "source": [
        "# Evaluating the agent's performance after training\r\n",
        "\r\n",
        "total_epochs, total_penalties = 0, 0\r\n",
        "episodes = 1  # change to however many episodes we'd like to test\r\n",
        "frames = []\r\n",
        "\r\n",
        "for _ in range(episodes):\r\n",
        "  state = env.reset()\r\n",
        "  epochs, penalties, reward = 0, 0, 0\r\n",
        "  done = False\r\n",
        "\r\n",
        "  while not done:\r\n",
        "    action = np.argmax(q_table[state])\r\n",
        "    state, reward, done, info = env.step(action)\r\n",
        "\r\n",
        "    if reward == -10:\r\n",
        "      penalties += 1\r\n",
        "\r\n",
        "    # Putting each rendered frame into a dict for the animation\r\n",
        "\r\n",
        "    frames.append({\r\n",
        "      'frame' : env.render(mode = 'ansi'),\r\n",
        "      'state' : state,\r\n",
        "      'action' : action,\r\n",
        "      'reward' : reward\r\n",
        "      }\r\n",
        "    )\r\n",
        "\r\n",
        "    epochs += 1\r\n",
        "\r\n",
        "  total_penalties += penalties\r\n",
        "  total_epochs = epochs\r\n",
        "\r\n",
        "print(f\"Results after {episodes} episodes:\")\r\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\r\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results after 1 episodes:\n",
            "Average timesteps per episode: 7.0\n",
            "Average penalties per episode: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2nOkrFkf6iA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ca41316-7b94-4d01-ab9d-40e2fed58a0f"
      },
      "source": [
        "print_frames(frames, 0.6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Timestep: 7\n",
            "State: 410\n",
            "Action: 5\n",
            "Reward: 20\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}